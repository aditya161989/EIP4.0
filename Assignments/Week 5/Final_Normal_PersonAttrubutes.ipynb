{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Normal_PersonAttrubutes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditya161989/EIP4.0/blob/master/Final_Normal_PersonAttrubutes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "7b0bf569-8f17-4631-a686-3bf4ab17f7c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "outputId": "5c85aac6-116c-4469-dc59-2366e6165ea0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "from keras import regularizers\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, SeparableConv2D\n",
        "from keras.layers import Activation, Flatten, Dense, Dropout, SpatialDropout2D, UpSampling2D, GlobalAveragePooling2D, AveragePooling2D\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "import numpy as np\n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kayy7BIUzJIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize model parameters\n",
        "\n",
        "# Training parameters\n",
        "epochs = 20\n",
        "data_augmentation = True\n",
        "\n",
        "# Weight loading required\n",
        "load_saved_weights = True\n",
        "\n",
        "# Subtracting pixel mean improves accuracy\n",
        "subtract_pixel_mean = True\n",
        "\n",
        "# Epoch count\n",
        "epoch_count = 20\n",
        "\n",
        "# Filepath for loading saved weights\n",
        "filepath=\"/content/gdrive/My Drive/Model/newaugweights-05-7.88.hdf5\"\n",
        "\n",
        "# Filepath for saving weights\n",
        "savefilepath=\"/content/gdrive/My Drive/Model/newaugweights-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
        "\n",
        "weight_decay = 4e-3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "62208904-ce56-41f0-8a66-9b58b6b04c54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13573, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmTqGEdtpZej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Since the model is being trained multiple times using best saved weights,\n",
        "# the train test split is saved in drive so that the model never trains on test data\n",
        "test_indices = list(np.load('/content/gdrive/My Drive/test_indices.npy'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9nQsNSgCmyS",
        "colab_type": "code",
        "outputId": "6a4b1d20-6fb3-480c-a08c-fbfb116ed065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(test_indices)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2036"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "d3309837-3131-484a-e40e-deb40c91bf01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>resized/5.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "0  resized/1.jpg              0  ...                        1              0\n",
              "1  resized/2.jpg              1  ...                        1              0\n",
              "2  resized/3.jpg              0  ...                        1              0\n",
              "3  resized/4.jpg              0  ...                        1              0\n",
              "4  resized/5.jpg              1  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdxHd-LpkwRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cut out implementation\n",
        "\n",
        "# p : the probability that random erasing is performed\n",
        "# s_l, s_h : minimum / maximum proportion of erased area against input image\n",
        "# r_1, r_2 : minimum / maximum aspect ratio of erased area\n",
        "# v_l, v_h : minimum / maximum value for erased area\n",
        "# pixel_level : pixel-level randomization for erased area\n",
        "\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.1, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        # print(image.shape)\n",
        "        if data_augmentation == True:\n",
        "          eraser = get_random_eraser(v_h = 1)\n",
        "          img_gen = ImageDataGenerator()\n",
        "          for idx in range(image.shape[0]):\n",
        "            image[idx] = image[idx].astype('float32') / 255\n",
        "            image[idx] = eraser(image[idx])\n",
        "            image[idx] = img_gen.apply_transform(image[idx], transform_parameters = {'tx': .1, 'ty': .1})\n",
        "        if index == (self.__len__() - 1):\n",
        "          self.on_epoch_end()\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3Hfnf6mC5Jb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_df = one_hot_df.iloc[test_indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spXPzvpvDNAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_indices = [i for i in range(0,one_hot_df.shape[0]) if i not in test_indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePqGOj_9Dtxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = one_hot_df.iloc[train_indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNgBfs5BD8lK",
        "colab_type": "code",
        "outputId": "21ffe7fa-b9e4-4526-894e-e56b61ea74e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11680, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "outputId": "0228d925-cf0d-4920-ae98-cd47fdcb9548",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>resized/5.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>resized/6.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "0  resized/1.jpg              0  ...                        1              0\n",
              "2  resized/3.jpg              0  ...                        1              0\n",
              "3  resized/4.jpg              0  ...                        1              0\n",
              "4  resized/5.jpg              1  ...                        1              0\n",
              "5  resized/6.jpg              0  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=64)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=64, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "08fb7013-3794-4718-bfaa-a21050e59db8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHTu3Eu9cYMD",
        "colab_type": "code",
        "outputId": "381d0317-65b6-4276-e29d-7dd9a151da0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# Transform the image to 56X56 ASAP\n",
        "inputlayer = Input(shape=(224,224,3))\n",
        "start = Conv2D(32, (3, 3),  kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(weight_decay), input_shape=(224,224,3), padding='same')(inputlayer)\n",
        "start1 = Conv2D(64, (3, 3),  kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(weight_decay), padding='same')(start)\n",
        "pool = AveragePooling2D()(start1)\n",
        "mid = Conv2D(32, (3, 3),  kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(weight_decay), padding='same')(pool)\n",
        "mid1 = Conv2D(64, (3, 3),  kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(weight_decay), padding='same')(mid)\n",
        "pool2 = AveragePooling2D()(mid1)\n",
        "end = Conv2D(8,1,1)(pool2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (1, 1))`\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRQhjufXyheg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def newBody(num_classes=10, name='demo'):\n",
        "  layer1 = SeparableConv2D(32, (3,3), padding='same', kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(weight_decay), input_shape=(56, 56, 8))(end)\n",
        "  layer1 = Activation('relu')(layer1)\n",
        "  layer1 = BatchNormalization()(layer1)\n",
        "\n",
        "  layer2 = SeparableConv2D(64, (3,3), padding='same', kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(weight_decay))(layer1)\n",
        "  layer2 = Activation('relu')(layer2)\n",
        "  layer2 = BatchNormalization()(layer2)\n",
        "\n",
        "  layer2 = AveragePooling2D()(layer2)#28\n",
        "  layer2 = Dropout(0.1)(layer2)\n",
        "  layer2 = SeparableConv2D(32,1,1,kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(weight_decay))(layer2)\n",
        "\n",
        "  layer3 = SeparableConv2D(32, (3,3), padding='same',kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(layer2)\n",
        "  layer3 = Activation('relu')(layer3)\n",
        "  layer3 = BatchNormalization()(layer3)\n",
        "\n",
        "  layer4 = SeparableConv2D(64, (3,3), padding='same',kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(layer3)\n",
        "  layer4 = Activation('relu')(layer4)\n",
        "  layer4 = BatchNormalization()(layer4)\n",
        "\n",
        "  layer4 = AveragePooling2D()(layer4)#14\n",
        "  layer4 = Dropout(0.1)(layer4)\n",
        "  layer4 = SeparableConv2D(32,1,1,kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(weight_decay))(layer4)\n",
        "\n",
        "  layer5 = SeparableConv2D(32, (3,3), padding='same',kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(layer4)\n",
        "  layer5 = Activation('relu')(layer5)\n",
        "  layer5 = BatchNormalization()(layer5)\n",
        "\n",
        "  layer6 = SeparableConv2D(64, (3,3), padding='same',kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(layer5)\n",
        "  layer6 = Activation('relu')(layer6)\n",
        "  layer6 = BatchNormalization()(layer6)\n",
        "\n",
        "  layer6 = AveragePooling2D()(layer6)#7\n",
        "  layer6 = Dropout(0.1)(layer6)\n",
        "  layer6 = SeparableConv2D(32,1,1,kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(weight_decay))(layer6)\n",
        "\n",
        "  layer7 = SeparableConv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay))(layer6)\n",
        "  layer7 = Activation('relu')(layer7)\n",
        "  layer7 = BatchNormalization()(layer7)\n",
        "\n",
        "  layer8 = SeparableConv2D(64, (3,3), padding='same',kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(layer7)\n",
        "  layer8 = Activation('relu')(layer8)\n",
        "  layer8 = BatchNormalization()(layer8)\n",
        "\n",
        "  layer8 = AveragePooling2D()(layer8)#7\n",
        "  layer8 = Dropout(0.1)(layer8)\n",
        "  layer8 = SeparableConv2D(32,1,1,kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(weight_decay))(layer8)\n",
        "\n",
        "  \n",
        "  layer9 = SeparableConv2D(32, (3,3), padding='same',kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(layer8)\n",
        "  layer9 = Activation('relu')(layer9)\n",
        "  layer9 = BatchNormalization()(layer9)\n",
        "\n",
        "  layer10 = SeparableConv2D(64, (3,3), padding='same',kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(layer9)\n",
        "  layer10 = Activation('relu')(layer10)\n",
        "  layer10 = BatchNormalization()(layer10)\n",
        "\n",
        "  layer11 = AveragePooling2D()(layer10)#3\n",
        "  layer11 = Dropout(0.1)(layer11)\n",
        "  layer11 = SeparableConv2D(32,1,1,kernel_regularizer=regularizers.l2(weight_decay))(layer11)  \n",
        "\n",
        "  layer12 = SeparableConv2D(64, (3,3), padding='same',kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(layer11)\n",
        "\n",
        "  layer13 = Flatten()(layer12)\n",
        "  outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal', name=f\"{name}_output\")(layer13)\n",
        "  return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "outputId": "18a5f996-e63c-4962-8868-f7b57f9b218f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "\n",
        "def build_head(name):\n",
        "    return newBody(num_classes=num_units[name], name= name)\n",
        "    \n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\")\n",
        "image_quality = build_head(\"image_quality\")\n",
        "age = build_head(\"age\")\n",
        "weight = build_head(\"weight\")\n",
        "bag = build_head(\"bag\")\n",
        "footwear = build_head(\"footwear\")\n",
        "emotion = build_head(\"emotion\")\n",
        "pose = build_head(\"pose\")\n",
        "\n",
        "# print(image_quality)\n",
        "\n",
        "model = Model(\n",
        "    inputs=inputlayer, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")\n",
        "\n",
        "print(model.count_params())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(32, (1, 1), kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(32, (1, 1), kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(32, (1, 1), kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(32, (1, 1), kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(32, (1, 1), kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "320835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a-ytd75lbVd",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBVxog_zpm30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scheduler(epoch, lr): #Implements cyclical learning rate\n",
        "  step_size = 4300 #One triangle is completed at the 46th epoch and the learning rate remains the base rate for the rest 4 epochs\n",
        "  iterations = epoch * 182\n",
        "  base_lr = 0.00025 #base learning rate\n",
        "  max_lr = .001 #max learning rate\n",
        "  cycle = np.floor(1+iterations/(2*step_size))\n",
        "  x = np.abs(iterations/step_size - 2*cycle + 1)\n",
        "  new_lr = base_lr + (max_lr-base_lr)*np.maximum(0, (1-x))\n",
        "  print((lr, new_lr))\n",
        "  if(round(lr, 5) <= base_lr and epoch > 1):\n",
        "    return base_lr \n",
        "  else:\n",
        "    return new_lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b7seNKtd6M2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "\n",
        "class MyLogger(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        avg = ( logs['val_gender_output_acc'] + logs['val_image_quality_output_acc'] + logs['val_age_output_acc'] + logs['val_weight_output_acc'] + logs['val_bag_output_acc'] + logs['val_footwear_output_acc'] + logs['val_pose_output_acc'] + logs['val_emotion_output_acc'])*100/8\n",
        "        print(avg) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKq6VQvJGBIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# checkpoint\n",
        "checkpoint = ModelCheckpoint(savefilepath, monitor='val_loss', save_weights_only=True, verbose=1)\n",
        "mylogger = MyLogger()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFWzQsaBeuUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "callbacks_list = [checkpoint, mylogger]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g71juUFp319S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def load_weights(model, filepath):\n",
        "#   return model.load_weights(filepath)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbS2nyHt9sdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss Definition\n",
        "# losses = {\n",
        "# \t\"gender_output\": \"binary_crossentropy\",\n",
        "# \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "# \t\"age_output\": \"categorical_crossentropy\",\n",
        "# \t\"weight_output\": \"categorical_crossentropy\",\n",
        "#   \"bag_output\": \"binary_crossentropy\",\n",
        "# \t\"footwear_output\": \"binary_crossentropy\",\n",
        "# \t\"emotion_output\": \"binary_crossentropy\",\n",
        "# \t\"pose_output\": \"binary_crossentropy\",\n",
        "\n",
        "# }\n",
        "loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0, \"weight_output\": 1.0, \"bag_output\": 1.0, \"footwear_output\": 1.0,\"emotion_output\": 1.0, \"pose_output\": 1.0}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRm7VmRCGbtK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute when weights have been saved\n",
        "\n",
        "opt = SGD(lr=0.00025, momentum=0.9)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3-SuEDJ-WZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if load_saved_weights == True:\n",
        "  model.load_weights(filepath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6DhMuMZZht5",
        "colab_type": "code",
        "outputId": "7fe9ed18-0ce8-4f97-a4b1-64884edf8ded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "results = model.evaluate_generator(valid_gen, verbose =1)\n",
        "dict(zip(model.metrics_names,results))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31/31 [==============================] - 25s 791ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age_output_acc': 0.3870967741935484,\n",
              " 'age_output_loss': 1.4171747546042166,\n",
              " 'bag_output_acc': 0.5700604838709677,\n",
              " 'bag_output_loss': 0.9117189261221117,\n",
              " 'emotion_output_acc': 0.7142137096774194,\n",
              " 'emotion_output_loss': 0.8971969427600983,\n",
              " 'footwear_output_acc': 0.5292338709677419,\n",
              " 'footwear_output_loss': 0.9682077342464078,\n",
              " 'gender_output_acc': 0.5866935483870968,\n",
              " 'gender_output_loss': 0.6707130670547485,\n",
              " 'image_quality_output_acc': 0.5383064516129032,\n",
              " 'image_quality_output_loss': 1.1432928423727713,\n",
              " 'loss': 7.880903736237557,\n",
              " 'pose_output_acc': 0.6421370967741935,\n",
              " 'pose_output_loss': 0.8890646792227223,\n",
              " 'weight_output_acc': 0.6436491935483871,\n",
              " 'weight_output_loss': 0.9705355532707707}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpxv41EyNmN4",
        "colab_type": "code",
        "outputId": "da7bbd96-3425-4031-bd54-08c9896bb40c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=20,\n",
        "    verbose=1,\n",
        "    callbacks = callbacks_list\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "182/182 [==============================] - 203s 1s/step - loss: 7.6646 - gender_output_loss: 0.6478 - image_quality_output_loss: 0.9500 - age_output_loss: 1.4144 - weight_output_loss: 0.9752 - bag_output_loss: 0.9057 - footwear_output_loss: 0.9576 - pose_output_loss: 0.8991 - emotion_output_loss: 0.9017 - gender_output_acc: 0.6035 - image_quality_output_acc: 0.5541 - age_output_acc: 0.4020 - weight_output_acc: 0.6360 - bag_output_acc: 0.5692 - footwear_output_acc: 0.5450 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7120 - val_loss: 7.7568 - val_gender_output_loss: 0.6739 - val_image_quality_output_loss: 1.0171 - val_age_output_loss: 1.4167 - val_weight_output_loss: 0.9643 - val_bag_output_loss: 0.9250 - val_footwear_output_loss: 0.9625 - val_pose_output_loss: 0.8893 - val_emotion_output_loss: 0.8952 - val_gender_output_acc: 0.5539 - val_image_quality_output_acc: 0.5373 - val_age_output_acc: 0.3866 - val_weight_output_acc: 0.6431 - val_bag_output_acc: 0.5741 - val_footwear_output_acc: 0.5358 - val_pose_output_acc: 0.6416 - val_emotion_output_acc: 0.7142\n",
            "\n",
            "Epoch 00001: saving model to /content/gdrive/My Drive/Model/newaugweights-01-7.76.hdf5\n",
            "57.33366935483871\n",
            "Epoch 2/20\n",
            "182/182 [==============================] - 152s 836ms/step - loss: 7.6668 - gender_output_loss: 0.6462 - image_quality_output_loss: 0.9505 - age_output_loss: 1.4147 - weight_output_loss: 0.9777 - bag_output_loss: 0.9123 - footwear_output_loss: 0.9561 - pose_output_loss: 0.9023 - emotion_output_loss: 0.8940 - gender_output_acc: 0.6094 - image_quality_output_acc: 0.5524 - age_output_acc: 0.4018 - weight_output_acc: 0.6341 - bag_output_acc: 0.5634 - footwear_output_acc: 0.5469 - pose_output_acc: 0.6174 - emotion_output_acc: 0.7156 - val_loss: 8.1680 - val_gender_output_loss: 0.9811 - val_image_quality_output_loss: 1.0278 - val_age_output_loss: 1.4174 - val_weight_output_loss: 0.9655 - val_bag_output_loss: 0.9868 - val_footwear_output_loss: 0.9908 - val_pose_output_loss: 0.8884 - val_emotion_output_loss: 0.8974 - val_gender_output_acc: 0.4567 - val_image_quality_output_acc: 0.5383 - val_age_output_acc: 0.3876 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.3614 - val_footwear_output_acc: 0.5161 - val_pose_output_acc: 0.6426 - val_emotion_output_acc: 0.7142\n",
            "\n",
            "Epoch 00002: saving model to /content/gdrive/My Drive/Model/newaugweights-02-8.17.hdf5\n",
            "53.24470766129033\n",
            "Epoch 3/20\n",
            "182/182 [==============================] - 154s 844ms/step - loss: 7.6677 - gender_output_loss: 0.6460 - image_quality_output_loss: 0.9490 - age_output_loss: 1.4160 - weight_output_loss: 0.9760 - bag_output_loss: 0.9074 - footwear_output_loss: 0.9552 - pose_output_loss: 0.9031 - emotion_output_loss: 0.9021 - gender_output_acc: 0.6113 - image_quality_output_acc: 0.5561 - age_output_acc: 0.3991 - weight_output_acc: 0.6340 - bag_output_acc: 0.5650 - footwear_output_acc: 0.5528 - pose_output_acc: 0.6150 - emotion_output_acc: 0.7121 - val_loss: 8.5179 - val_gender_output_loss: 0.9742 - val_image_quality_output_loss: 1.3928 - val_age_output_loss: 1.4185 - val_weight_output_loss: 0.9678 - val_bag_output_loss: 0.9977 - val_footwear_output_loss: 0.9601 - val_pose_output_loss: 0.8873 - val_emotion_output_loss: 0.9066 - val_gender_output_acc: 0.5494 - val_image_quality_output_acc: 0.2873 - val_age_output_acc: 0.3881 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.5711 - val_footwear_output_acc: 0.5333 - val_pose_output_acc: 0.6436 - val_emotion_output_acc: 0.7142\n",
            "\n",
            "Epoch 00003: saving model to /content/gdrive/My Drive/Model/newaugweights-03-8.52.hdf5\n",
            "54.120463709677416\n",
            "Epoch 4/20\n",
            "182/182 [==============================] - 154s 848ms/step - loss: 7.6659 - gender_output_loss: 0.6474 - image_quality_output_loss: 0.9482 - age_output_loss: 1.4143 - weight_output_loss: 0.9738 - bag_output_loss: 0.9095 - footwear_output_loss: 0.9539 - pose_output_loss: 0.9013 - emotion_output_loss: 0.9049 - gender_output_acc: 0.6055 - image_quality_output_acc: 0.5573 - age_output_acc: 0.4020 - weight_output_acc: 0.6352 - bag_output_acc: 0.5633 - footwear_output_acc: 0.5509 - pose_output_acc: 0.6176 - emotion_output_acc: 0.7107 - val_loss: 9.0567 - val_gender_output_loss: 1.2489 - val_image_quality_output_loss: 1.5737 - val_age_output_loss: 1.4208 - val_weight_output_loss: 0.9696 - val_bag_output_loss: 1.0587 - val_footwear_output_loss: 0.9758 - val_pose_output_loss: 0.8896 - val_emotion_output_loss: 0.9069 - val_gender_output_acc: 0.5479 - val_image_quality_output_acc: 0.2858 - val_age_output_acc: 0.3881 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.5706 - val_footwear_output_acc: 0.5222 - val_pose_output_acc: 0.6411 - val_emotion_output_acc: 0.7142\n",
            "\n",
            "Epoch 00004: saving model to /content/gdrive/My Drive/Model/newaugweights-04-9.06.hdf5\n",
            "53.90625\n",
            "Epoch 5/20\n",
            "182/182 [==============================] - 154s 844ms/step - loss: 7.6656 - gender_output_loss: 0.6472 - image_quality_output_loss: 0.9483 - age_output_loss: 1.4141 - weight_output_loss: 0.9748 - bag_output_loss: 0.9096 - footwear_output_loss: 0.9559 - pose_output_loss: 0.8995 - emotion_output_loss: 0.9035 - gender_output_acc: 0.6096 - image_quality_output_acc: 0.5563 - age_output_acc: 0.4008 - weight_output_acc: 0.6345 - bag_output_acc: 0.5621 - footwear_output_acc: 0.5466 - pose_output_acc: 0.6176 - emotion_output_acc: 0.7114 - val_loss: 8.4613 - val_gender_output_loss: 1.1391 - val_image_quality_output_loss: 1.0917 - val_age_output_loss: 1.4189 - val_weight_output_loss: 0.9691 - val_bag_output_loss: 1.0550 - val_footwear_output_loss: 0.9840 - val_pose_output_loss: 0.8924 - val_emotion_output_loss: 0.8984 - val_gender_output_acc: 0.4506 - val_image_quality_output_acc: 0.5383 - val_age_output_acc: 0.3871 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.3422 - val_footwear_output_acc: 0.5237 - val_pose_output_acc: 0.6411 - val_emotion_output_acc: 0.7142\n",
            "\n",
            "Epoch 00005: saving model to /content/gdrive/My Drive/Model/newaugweights-05-8.46.hdf5\n",
            "52.99899193548387\n",
            "Epoch 6/20\n",
            "182/182 [==============================] - 152s 838ms/step - loss: 7.6563 - gender_output_loss: 0.6446 - image_quality_output_loss: 0.9482 - age_output_loss: 1.4167 - weight_output_loss: 0.9737 - bag_output_loss: 0.9059 - footwear_output_loss: 0.9532 - pose_output_loss: 0.8999 - emotion_output_loss: 0.9015 - gender_output_acc: 0.6101 - image_quality_output_acc: 0.5566 - age_output_acc: 0.4001 - weight_output_acc: 0.6344 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5525 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7126 - val_loss: 9.1947 - val_gender_output_loss: 1.2202 - val_image_quality_output_loss: 1.6929 - val_age_output_loss: 1.4228 - val_weight_output_loss: 0.9702 - val_bag_output_loss: 1.0906 - val_footwear_output_loss: 0.9871 - val_pose_output_loss: 0.8910 - val_emotion_output_loss: 0.9071 - val_gender_output_acc: 0.5479 - val_image_quality_output_acc: 0.2858 - val_age_output_acc: 0.3886 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.5706 - val_footwear_output_acc: 0.5086 - val_pose_output_acc: 0.6426 - val_emotion_output_acc: 0.7142\n",
            "\n",
            "Epoch 00006: saving model to /content/gdrive/My Drive/Model/newaugweights-06-9.19.hdf5\n",
            "53.76134072580645\n",
            "Epoch 7/20\n",
            "182/182 [==============================] - 154s 844ms/step - loss: 7.6648 - gender_output_loss: 0.6450 - image_quality_output_loss: 0.9480 - age_output_loss: 1.4114 - weight_output_loss: 0.9730 - bag_output_loss: 0.9091 - footwear_output_loss: 0.9583 - pose_output_loss: 0.8973 - emotion_output_loss: 0.9100 - gender_output_acc: 0.6099 - image_quality_output_acc: 0.5573 - age_output_acc: 0.4044 - weight_output_acc: 0.6363 - bag_output_acc: 0.5645 - footwear_output_acc: 0.5458 - pose_output_acc: 0.6217 - emotion_output_acc: 0.7084 - val_loss: 7.8659 - val_gender_output_loss: 0.7102 - val_image_quality_output_loss: 1.0703 - val_age_output_loss: 1.4174 - val_weight_output_loss: 0.9641 - val_bag_output_loss: 0.9297 - val_footwear_output_loss: 0.9761 - val_pose_output_loss: 0.8891 - val_emotion_output_loss: 0.8964 - val_gender_output_acc: 0.5257 - val_image_quality_output_acc: 0.5373 - val_age_output_acc: 0.3876 - val_weight_output_acc: 0.6431 - val_bag_output_acc: 0.5585 - val_footwear_output_acc: 0.5307 - val_pose_output_acc: 0.6416 - val_emotion_output_acc: 0.7142\n",
            "\n",
            "Epoch 00006: saving model to /content/gdrive/My Drive/Model/newaugweights-06-9.19.hdf5\n",
            "\n",
            "Epoch 00007: saving model to /content/gdrive/My Drive/Model/newaugweights-07-7.87.hdf5\n",
            "56.7351310483871\n",
            "Epoch 8/20\n",
            "182/182 [==============================] - 156s 854ms/step - loss: 7.6585 - gender_output_loss: 0.6442 - image_quality_output_loss: 0.9529 - age_output_loss: 1.4150 - weight_output_loss: 0.9697 - bag_output_loss: 0.9039 - footwear_output_loss: 0.9572 - pose_output_loss: 0.8966 - emotion_output_loss: 0.9065 - gender_output_acc: 0.6077 - image_quality_output_acc: 0.5507 - age_output_acc: 0.4018 - weight_output_acc: 0.6377 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5461 - pose_output_acc: 0.6197 - emotion_output_acc: 0.7093 - val_loss: 8.5120 - val_gender_output_loss: 1.1500 - val_image_quality_output_loss: 1.1266 - val_age_output_loss: 1.4198 - val_weight_output_loss: 0.9703 - val_bag_output_loss: 1.0562 - val_footwear_output_loss: 0.9863 - val_pose_output_loss: 0.8907 - val_emotion_output_loss: 0.8994 - val_gender_output_acc: 0.4521 - val_image_quality_output_acc: 0.4718 - val_age_output_acc: 0.3876 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.3432 - val_footwear_output_acc: 0.5181 - val_pose_output_acc: 0.6431 - val_emotion_output_acc: 0.7142\n",
            "\n",
            "Epoch 00008: saving model to /content/gdrive/My Drive/Model/newaugweights-08-8.51.hdf5\n",
            "52.161038306451616\n",
            "Epoch 9/20\n",
            "182/182 [==============================] - 155s 852ms/step - loss: 7.6626 - gender_output_loss: 0.6430 - image_quality_output_loss: 0.9501 - age_output_loss: 1.4196 - weight_output_loss: 0.9742 - bag_output_loss: 0.9105 - footwear_output_loss: 0.9546 - pose_output_loss: 0.8989 - emotion_output_loss: 0.8992 - gender_output_acc: 0.6143 - image_quality_output_acc: 0.5547 - age_output_acc: 0.3965 - weight_output_acc: 0.6338 - bag_output_acc: 0.5640 - footwear_output_acc: 0.5513 - pose_output_acc: 0.6187 - emotion_output_acc: 0.7130 - val_loss: 8.3786 - val_gender_output_loss: 1.1436 - val_image_quality_output_loss: 1.0389 - val_age_output_loss: 1.4194 - val_weight_output_loss: 0.9658 - val_bag_output_loss: 1.0478 - val_footwear_output_loss: 0.9635 - val_pose_output_loss: 0.8879 - val_emotion_output_loss: 0.8992 - val_gender_output_acc: 0.4526 - val_image_quality_output_acc: 0.5383 - val_age_output_acc: 0.3876 - val_weight_output_acc: 0.6431 - val_bag_output_acc: 0.3427 - val_footwear_output_acc: 0.5383 - val_pose_output_acc: 0.6421 - val_emotion_output_acc: 0.7142\n",
            "\n",
            "Epoch 00009: saving model to /content/gdrive/My Drive/Model/newaugweights-09-8.38.hdf5\n",
            "53.23840725806451\n",
            "Epoch 10/20\n",
            "181/182 [============================>.] - ETA: 0s - loss: 7.6629 - gender_output_loss: 0.6417 - image_quality_output_loss: 0.9462 - age_output_loss: 1.4138 - weight_output_loss: 0.9738 - bag_output_loss: 0.9070 - footwear_output_loss: 0.9572 - pose_output_loss: 0.9064 - emotion_output_loss: 0.9045 - gender_output_acc: 0.6127 - image_quality_output_acc: 0.5563 - age_output_acc: 0.4021 - weight_output_acc: 0.6360 - bag_output_acc: 0.5650 - footwear_output_acc: 0.5460 - pose_output_acc: 0.6121 - emotion_output_acc: 0.7111\n",
            "182/182 [==============================] - 153s 843ms/step - loss: 7.6620 - gender_output_loss: 0.6415 - image_quality_output_loss: 0.9466 - age_output_loss: 1.4132 - weight_output_loss: 0.9739 - bag_output_loss: 0.9070 - footwear_output_loss: 0.9569 - pose_output_loss: 0.9058 - emotion_output_loss: 0.9046 - gender_output_acc: 0.6125 - image_quality_output_acc: 0.5562 - age_output_acc: 0.4023 - weight_output_acc: 0.6360 - bag_output_acc: 0.5651 - footwear_output_acc: 0.5463 - pose_output_acc: 0.6125 - emotion_output_acc: 0.7109 - val_loss: 8.3866 - val_gender_output_loss: 0.9418 - val_image_quality_output_loss: 1.2889 - val_age_output_loss: 1.4171 - val_weight_output_loss: 0.9826 - val_bag_output_loss: 0.9813 - val_footwear_output_loss: 0.9633 - val_pose_output_loss: 0.8912 - val_emotion_output_loss: 0.9078 - val_gender_output_acc: 0.5504 - val_image_quality_output_acc: 0.2928 - val_age_output_acc: 0.3886 - val_weight_output_acc: 0.6436 - val_bag_output_acc: 0.5716 - val_footwear_output_acc: 0.5277 - val_pose_output_acc: 0.6447 - val_emotion_output_acc: 0.7142\n",
            "\n",
            "Epoch 00010: saving model to /content/gdrive/My Drive/Model/newaugweights-10-8.39.hdf5\n",
            "54.17086693548387\n",
            "Epoch 11/20\n",
            "182/182 [==============================] - 154s 844ms/step - loss: 7.6651 - gender_output_loss: 0.6446 - image_quality_output_loss: 0.9527 - age_output_loss: 1.4107 - weight_output_loss: 0.9736 - bag_output_loss: 0.9101 - footwear_output_loss: 0.9552 - pose_output_loss: 0.9012 - emotion_output_loss: 0.9046 - gender_output_acc: 0.6107 - image_quality_output_acc: 0.5530 - age_output_acc: 0.3999 - weight_output_acc: 0.6363 - bag_output_acc: 0.5636 - footwear_output_acc: 0.5502 - pose_output_acc: 0.6172 - emotion_output_acc: 0.7109 - val_loss: 8.1854 - val_gender_output_loss: 0.9682 - val_image_quality_output_loss: 1.0391 - val_age_output_loss: 1.4189 - val_weight_output_loss: 0.9667 - val_bag_output_loss: 1.0082 - val_footwear_output_loss: 0.9820 - val_pose_output_loss: 0.8898 - val_emotion_output_loss: 0.9001 - val_gender_output_acc: 0.4567 - val_image_quality_output_acc: 0.5383 - val_age_output_acc: 0.3876 - val_weight_output_acc: 0.6431 - val_bag_output_acc: 0.3533 - val_footwear_output_acc: 0.5207 - val_pose_output_acc: 0.6431 - val_emotion_output_acc: 0.7142\n",
            "\n",
            "Epoch 00011: saving model to /content/gdrive/My Drive/Model/newaugweights-11-8.19.hdf5\n",
            "53.21320564516129\n",
            "Epoch 12/20\n",
            "182/182 [==============================] - 153s 841ms/step - loss: 7.6573 - gender_output_loss: 0.6457 - image_quality_output_loss: 0.9473 - age_output_loss: 1.4127 - weight_output_loss: 0.9706 - bag_output_loss: 0.9096 - footwear_output_loss: 0.9549 - pose_output_loss: 0.9018 - emotion_output_loss: 0.9024 - gender_output_acc: 0.6103 - image_quality_output_acc: 0.5570 - age_output_acc: 0.4015 - weight_output_acc: 0.6359 - bag_output_acc: 0.5660 - footwear_output_acc: 0.5461 - pose_output_acc: 0.6151 - emotion_output_acc: 0.7117 - val_loss: 9.5707 - val_gender_output_loss: 1.0684 - val_image_quality_output_loss: 1.9020 - val_age_output_loss: 1.4363 - val_weight_output_loss: 0.9731 - val_bag_output_loss: 1.1558 - val_footwear_output_loss: 1.2246 - val_pose_output_loss: 0.8898 - val_emotion_output_loss: 0.9083 - val_gender_output_acc: 0.5479 - val_image_quality_output_acc: 0.2853 - val_age_output_acc: 0.3876 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.5701 - val_footwear_output_acc: 0.4330 - val_pose_output_acc: 0.6426 - val_emotion_output_acc: 0.7142\n",
            "\n",
            "Epoch 00012: saving model to /content/gdrive/My Drive/Model/newaugweights-12-9.57.hdf5\n",
            "52.79107862903226\n",
            "Epoch 13/20\n",
            "182/182 [==============================] - 152s 838ms/step - loss: 7.6533 - gender_output_loss: 0.6451 - image_quality_output_loss: 0.9449 - age_output_loss: 1.4155 - weight_output_loss: 0.9705 - bag_output_loss: 0.9102 - footwear_output_loss: 0.9595 - pose_output_loss: 0.8956 - emotion_output_loss: 0.8996 - gender_output_acc: 0.6110 - image_quality_output_acc: 0.5592 - age_output_acc: 0.4017 - weight_output_acc: 0.6370 - bag_output_acc: 0.5591 - footwear_output_acc: 0.5464 - pose_output_acc: 0.6191 - emotion_output_acc: 0.7133 - val_loss: 9.4553 - val_gender_output_loss: 1.1111 - val_image_quality_output_loss: 1.7988 - val_age_output_loss: 1.4301 - val_weight_output_loss: 0.9722 - val_bag_output_loss: 1.1240 - val_footwear_output_loss: 1.2082 - val_pose_output_loss: 0.8891 - val_emotion_output_loss: 0.9095 - val_gender_output_acc: 0.5479 - val_image_quality_output_acc: 0.2858 - val_age_output_acc: 0.3866 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.5706 - val_footwear_output_acc: 0.4350 - val_pose_output_acc: 0.6431 - val_emotion_output_acc: 0.7142\n",
            "\n",
            "Epoch 00013: saving model to /content/gdrive/My Drive/Model/newaugweights-13-9.46.hdf5\n",
            "52.82258064516129\n",
            "Epoch 14/20\n",
            "182/182 [==============================] - 153s 841ms/step - loss: 7.6593 - gender_output_loss: 0.6419 - image_quality_output_loss: 0.9505 - age_output_loss: 1.4135 - weight_output_loss: 0.9747 - bag_output_loss: 0.9065 - footwear_output_loss: 0.9571 - pose_output_loss: 0.8986 - emotion_output_loss: 0.9042 - gender_output_acc: 0.6125 - image_quality_output_acc: 0.5557 - age_output_acc: 0.3986 - weight_output_acc: 0.6344 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5485 - pose_output_acc: 0.6165 - emotion_output_acc: 0.7103 - val_loss: 9.1816 - val_gender_output_loss: 1.2226 - val_image_quality_output_loss: 1.6052 - val_age_output_loss: 1.4249 - val_weight_output_loss: 0.9692 - val_bag_output_loss: 1.1032 - val_footwear_output_loss: 1.0464 - val_pose_output_loss: 0.8878 - val_emotion_output_loss: 0.9101 - val_gender_output_acc: 0.5484 - val_image_quality_output_acc: 0.2858 - val_age_output_acc: 0.3876 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.5706 - val_footwear_output_acc: 0.4677 - val_pose_output_acc: 0.6436 - val_emotion_output_acc: 0.7142\n",
            "\n",
            "Epoch 00014: saving model to /content/gdrive/My Drive/Model/newaugweights-14-9.18.hdf5\n",
            "53.25730846774193\n",
            "Epoch 15/20\n",
            "182/182 [==============================] - 154s 844ms/step - loss: 7.6564 - gender_output_loss: 0.6437 - image_quality_output_loss: 0.9483 - age_output_loss: 1.4148 - weight_output_loss: 0.9730 - bag_output_loss: 0.9091 - footwear_output_loss: 0.9523 - pose_output_loss: 0.8995 - emotion_output_loss: 0.9034 - gender_output_acc: 0.6101 - image_quality_output_acc: 0.5551 - age_output_acc: 0.4009 - weight_output_acc: 0.6345 - bag_output_acc: 0.5638 - footwear_output_acc: 0.5528 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7112 - val_loss: 9.4917 - val_gender_output_loss: 1.5726 - val_image_quality_output_loss: 1.2977 - val_age_output_loss: 1.4212 - val_weight_output_loss: 0.9732 - val_bag_output_loss: 1.4164 - val_footwear_output_loss: 0.9954 - val_pose_output_loss: 0.9036 - val_emotion_output_loss: 0.8994 - val_gender_output_acc: 0.4521 - val_image_quality_output_acc: 0.3019 - val_age_output_acc: 0.3866 - val_weight_output_acc: 0.6431 - val_bag_output_acc: 0.3327 - val_footwear_output_acc: 0.5141 - val_pose_output_acc: 0.6416 - val_emotion_output_acc: 0.7142\n",
            "\n",
            "Epoch 00015: saving model to /content/gdrive/My Drive/Model/newaugweights-15-9.49.hdf5\n",
            "49.829889112903224\n",
            "Epoch 16/20\n",
            "182/182 [==============================] - 153s 840ms/step - loss: 7.6476 - gender_output_loss: 0.6435 - image_quality_output_loss: 0.9454 - age_output_loss: 1.4122 - weight_output_loss: 0.9736 - bag_output_loss: 0.9088 - footwear_output_loss: 0.9553 - pose_output_loss: 0.8947 - emotion_output_loss: 0.9019 - gender_output_acc: 0.6132 - image_quality_output_acc: 0.5566 - age_output_acc: 0.4020 - weight_output_acc: 0.6363 - bag_output_acc: 0.5643 - footwear_output_acc: 0.5487 - pose_output_acc: 0.6217 - emotion_output_acc: 0.7121 - val_loss: 8.9360 - val_gender_output_loss: 1.3657 - val_image_quality_output_loss: 1.2114 - val_age_output_loss: 1.4218 - val_weight_output_loss: 0.9714 - val_bag_output_loss: 1.1667 - val_footwear_output_loss: 0.9935 - val_pose_output_loss: 0.8926 - val_emotion_output_loss: 0.9007 - val_gender_output_acc: 0.4516 - val_image_quality_output_acc: 0.3327 - val_age_output_acc: 0.3866 - val_weight_output_acc: 0.6431 - val_bag_output_acc: 0.3337 - val_footwear_output_acc: 0.5146 - val_pose_output_acc: 0.6447 - val_emotion_output_acc: 0.7142\n",
            "\n",
            "Epoch 00016: saving model to /content/gdrive/My Drive/Model/newaugweights-16-8.94.hdf5\n",
            "50.264616935483865\n",
            "Epoch 17/20\n",
            "181/182 [============================>.] - ETA: 0s - loss: 7.6477 - gender_output_loss: 0.6425 - image_quality_output_loss: 0.9437 - age_output_loss: 1.4105 - weight_output_loss: 0.9723 - bag_output_loss: 0.9113 - footwear_output_loss: 0.9532 - pose_output_loss: 0.8971 - emotion_output_loss: 0.9051 - gender_output_acc: 0.6171 - image_quality_output_acc: 0.5605 - age_output_acc: 0.4006 - weight_output_acc: 0.6366 - bag_output_acc: 0.5601 - footwear_output_acc: 0.5508 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7099 - 153s 840ms/step - loss: 7.6476 - gender_output_loss: 0.6435 - image_quality_output_loss: 0.9454 - age_output_loss: 1.4122 - weight_output_loss: 0.9736 - bag_output_loss: 0.9088 - footwear_output_loss: 0.9553 - pose_output_loss: 0.8947 - emotion_output_loss: 0.9019 - gender_output_acc: 0.6132 - image_quality_output_acc: 0.5566 - age_output_acc: 0.4020 - weight_output_acc: 0.6363 - bag_output_acc: 0.5643 - footwear_output_acc: 0.5487 - pose_output_acc: 0.6217 - emotion_output_acc: 0.7121 - val_loss: 8.9360 - val_gender_output_loss: 1.3657 - val_image_quality_output_loss: 1.2114 - val_age_output_loss: 1.4218 - val_weight_output_loss: 0.9714 - val_bag_output_loss: 1.1667 - val_footwear_output_loss: 0.9935 - val_pose_output_loss: 0.8926 - val_emotion_output_loss: 0.9007 - val_gender_output_acc: 0.4516 - val_image_quality_output_acc: 0.3327 - val_age_output_acc: 0.3866 - val_weight_output_acc: 0.6431 - val_bag_output_acc: 0.3337 - val_footwear_output_acc: 0.5146 - val_pose_output_acc: 0.6447 - val_emotion_output_acc: 0.7142\n",
            "182/182 [==============================] - 153s 838ms/step - loss: 7.6458 - gender_output_loss: 0.6426 - image_quality_output_loss: 0.9437 - age_output_loss: 1.4102 - weight_output_loss: 0.9718 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9531 - pose_output_loss: 0.8963 - emotion_output_loss: 0.9049 - gender_output_acc: 0.6168 - image_quality_output_acc: 0.5602 - age_output_acc: 0.4010 - weight_output_acc: 0.6368 - bag_output_acc: 0.5600 - footwear_output_acc: 0.5509 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7101 - val_loss: 7.8494 - val_gender_output_loss: 0.7559 - val_image_quality_output_loss: 1.0067 - val_age_output_loss: 1.4185 - val_weight_output_loss: 0.9643 - val_bag_output_loss: 0.9364 - val_footwear_output_loss: 0.9673 - val_pose_output_loss: 0.8905 - val_emotion_output_loss: 0.8977 - val_gender_output_acc: 0.5091 - val_image_quality_output_acc: 0.5388 - val_age_output_acc: 0.3871 - val_weight_output_acc: 0.6436 - val_bag_output_acc: 0.4536 - val_footwear_output_acc: 0.5287 - val_pose_output_acc: 0.6447 - val_emotion_output_acc: 0.7142\n",
            "\n",
            "Epoch 00017: saving model to /content/gdrive/My Drive/Model/newaugweights-17-7.85.hdf5\n",
            "55.248235887096776\n",
            "Epoch 18/20\n",
            "181/182 [============================>.] - ETA: 0s - loss: 7.6659 - gender_output_loss: 0.6416 - image_quality_output_loss: 0.9483 - age_output_loss: 1.4180 - weight_output_loss: 0.9729 - bag_output_loss: 0.9108 - footwear_output_loss: 0.9575 - pose_output_loss: 0.8985 - emotion_output_loss: 0.9061 - gender_output_acc: 0.6156 - image_quality_output_acc: 0.5568 - age_output_acc: 0.3967 - weight_output_acc: 0.6352 - bag_output_acc: 0.5620 - footwear_output_acc: 0.5479 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7099\n",
            "Epoch 00017: saving model to /content/gdrive/My Drive/Model/newaugweights-17-7.85.hdf5\n",
            "182/182 [==============================] - 153s 842ms/step - loss: 7.6669 - gender_output_loss: 0.6415 - image_quality_output_loss: 0.9479 - age_output_loss: 1.4173 - weight_output_loss: 0.9739 - bag_output_loss: 0.9108 - footwear_output_loss: 0.9583 - pose_output_loss: 0.8984 - emotion_output_loss: 0.9066 - gender_output_acc: 0.6153 - image_quality_output_acc: 0.5570 - age_output_acc: 0.3973 - weight_output_acc: 0.6347 - bag_output_acc: 0.5620 - footwear_output_acc: 0.5472 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7096 - val_loss: 9.1015 - val_gender_output_loss: 1.1691 - val_image_quality_output_loss: 1.6236 - val_age_output_loss: 1.4246 - val_weight_output_loss: 0.9679 - val_bag_output_loss: 1.0987 - val_footwear_output_loss: 1.0059 - val_pose_output_loss: 0.8883 - val_emotion_output_loss: 0.9115 - val_gender_output_acc: 0.5489 - val_image_quality_output_acc: 0.2858 - val_age_output_acc: 0.3871 - val_weight_output_acc: 0.6431 - val_bag_output_acc: 0.5706 - val_footwear_output_acc: 0.4924 - val_pose_output_acc: 0.6431 - val_emotion_output_acc: 0.7142\n",
            "\n",
            "Epoch 00018: saving model to /content/gdrive/My Drive/Model/newaugweights-18-9.10.hdf5\n",
            "53.56602822580645\n",
            "Epoch 19/20\n",
            "182/182 [==============================] - 153s 842ms/step - loss: 7.6527 - gender_output_loss: 0.6422 - image_quality_output_loss: 0.9486 - age_output_loss: 1.4150 - weight_output_loss: 0.9734 - bag_output_loss: 0.9083 - footwear_output_loss: 0.9544 - pose_output_loss: 0.8975 - emotion_output_loss: 0.9012 - gender_output_acc: 0.6149 - image_quality_output_acc: 0.5549 - age_output_acc: 0.3997 - weight_output_acc: 0.6346 - bag_output_acc: 0.5649 - footwear_output_acc: 0.5491 - pose_output_acc: 0.6176 - emotion_output_acc: 0.7127 - val_loss: 8.0911 - val_gender_output_loss: 0.9320 - val_image_quality_output_loss: 1.0201 - val_age_output_loss: 1.4176 - val_weight_output_loss: 0.9656 - val_bag_output_loss: 0.9890 - val_footwear_output_loss: 0.9644 - val_pose_output_loss: 0.8898 - val_emotion_output_loss: 0.9007 - val_gender_output_acc: 0.4652 - val_image_quality_output_acc: 0.5373 - val_age_output_acc: 0.3876 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.3639 - val_footwear_output_acc: 0.5312 - val_pose_output_acc: 0.6436 - val_emotion_output_acc: 0.7142\n",
            "\n",
            "Epoch 00018: saving model to /content/gdrive/My Drive/Model/newaugweights-18-9.10.hdf5\n",
            "\n",
            "Epoch 00019: saving model to /content/gdrive/My Drive/Model/newaugweights-19-8.09.hdf5\n",
            "53.57232862903225\n",
            "Epoch 20/20\n",
            "182/182 [==============================] - 154s 844ms/step - loss: 7.6620 - gender_output_loss: 0.6427 - image_quality_output_loss: 0.9495 - age_output_loss: 1.4157 - weight_output_loss: 0.9755 - bag_output_loss: 0.9080 - footwear_output_loss: 0.9552 - pose_output_loss: 0.8971 - emotion_output_loss: 0.9062 - gender_output_acc: 0.6134 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3990 - weight_output_acc: 0.6349 - bag_output_acc: 0.5628 - footwear_output_acc: 0.5487 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7100 - val_loss: 9.7533 - val_gender_output_loss: 1.0555 - val_image_quality_output_loss: 1.9439 - val_age_output_loss: 1.4493 - val_weight_output_loss: 0.9725 - val_bag_output_loss: 1.2377 - val_footwear_output_loss: 1.2826 - val_pose_output_loss: 0.8888 - val_emotion_output_loss: 0.9112 - val_gender_output_acc: 0.5479 - val_image_quality_output_acc: 0.2853 - val_age_output_acc: 0.3896 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.5711 - val_footwear_output_acc: 0.4244 - val_pose_output_acc: 0.6431 - val_emotion_output_acc: 0.7142\n",
            "\n",
            "Epoch 00020: saving model to /content/gdrive/My Drive/Model/newaugweights-20-9.75.hdf5\n",
            "52.728074596774185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f56ab291588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI1hJb4qM6OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}